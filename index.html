<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhiqiang Yan</title>

  <meta name="author" content="Zhiqiang Yan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cmu-seal-r.png">
<style>
.justify-text {
  text-align: justify;
}
</style>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhiqiang Yan</name>
                  </p>
                  <p>I'm currently a research fellow at National University of Singapore (NUS), 
			  working closely with Prof. <a href="https://www.comp.nus.edu.sg/~leegh/index.html">Gim Hee Lee</a> in 3D computer vision. 
			  Before that, I obtained my PhD degree at <a href="http://www.patternrecognition.asia/">PCALab</a>, <a
                      href="https://www.njust.edu.cn/">Nanjing University of Science and Technology</a> (NJUST) in June 2024, advised by Prof.  
			  <a
                      href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a> 
			  and co-advised by Prof. 
                    <a href="https://sites.google.com/view/junlineu/">Jun Li. </a> 
			  In June 2018, I received the Bachelor degree from NJUST. 
			  Welcome to contact me for discussion and cooperation. 
                  <p style="text-align:center">
                    <a href="mailto:yanzq@nus.edu.sg"> Email </a> &nbsp/&nbsp
                    <a href="images/CV_Yanzq.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://github.com/yanzq95"> GitHub </a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=hnrkzIEAAAAJ&hl=zh-CN&oi=sra"> Google Scholar </a> &nbsp/&nbsp
		    <!-- <a href="images/wechat_yanzq.jpg"> WeChat </a> -->
                  </p>
		  <p>
			  <!-- <font color='red'>I am looking for a postdoctoral position that will allow me to continue my research and expand my academic skills. I have graduated in June 2024. 
				  If you are interested in my profile, please contact me. Thank you.</font> -->
		  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a><img style="width:200px;max-width:100%" alt="profile photo" src="images/Staff_Photo.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My research interests include computer vision and machine learning, especially on depth estimation, depth completion, depth super-resolution, and NeRF rendering. These tasks are crucial for various applications, 
			  such as self-driving, robotic vision, and related 3D visual perception. I am also fascinated by the task of 3D occupancy prediction.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


	<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/LIAR/LIAR.png" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2505.20641">See through the Dark: Learning Illumination-affined
 Representations for Nighttime Occupancy Prediction</a>
                  </papertitle>
                  <br>
                        Yuan Wu*</a>, 
			<strong>Zhiqiang Yan* &#9993</strong>, 
			Yigong Zhang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>arXiv</em>, 2025<!--, <a href="projectpage/DHD/index.html">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  Existing vision-based methods perform well on daytime benchmarks but struggle in nighttime scenarios due to limited visibility and challenging lighting conditions. 
			  For the first time, we introduce LIAR, a novel framework that learns illumination-affined representations for nighttime occupacy prediction.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


	    <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/EventDC/data_case.png" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2505.13279">Event-Driven Dynamic Scene Depth Completion</a>
                  </papertitle>
                  <br>
			<strong>Zhiqiang Yan</strong>, 
			<a href="https://gogojjh.github.io/">Jianhao Jiao</a>, 
			<a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang</a>, 
			<a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
                  <br> <br>
                  <em>arXiv</em>, 2025<!--, <a href="https://github.com/yanzq95/DuCos">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  We introduce EventDC, the first depth completion framework that tackles the challenges of dynamic scenes by harnessing the unique strengths of event data. 
                          To mitigate the adverse effects of fast ego-motion and object motion, EventDC incorporates two event-driven modules. Furthermore, to support research in 
			  this area, we construct the first benchmark for event-based depth completion comprising one real-world and two synthetic datasets. 
			  </div>
			  </p>
                </td>
              </tr>
          </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DuCos/DuCos.png" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2503.04171">DuCos: Duality Constrained Depth Super-Resolution via Foundation Model</a>
                  </papertitle>
                  <br>
			<strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang</a>, 
			<a href="https://www.haoyed.com/">Haoye Dong</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a>,
			<a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
                  <br> <br>
                  <em>arXiv</em>, 2025<!--, <a href="https://github.com/yanzq95/DuCos">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  We introduce DuCos, a novel depth super-resolution framework grounded in Lagrangian duality theory, 
			  offering a flexible integration of multiple constraints and reconstruction objectives to enhance accuracy and robustness. 
			  Our DuCos is the first to significantly improve generalization across diverse scenarios with foundation models as prompts. 
			  Crucially, these prompts are seamlessly embedded into the Lagrangian constraint term, forming a synergistic and principled framework.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>

		
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DORNet/Params_Time.png" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2410.11666">DORNet: ADegradation Oriented and Regularized Network for Blind Depth Super-Resolution</a>
                  </papertitle>
                  <br>
                  <a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang*</a>, 
			  <strong>Zhiqiang Yan*</strong> &#9993, 
			  <a href="https://jspan.github.io/">Jinshan Pan</a>, 
			  <a href="https://guangweigao.github.io/">Guangwei Gao</a>,
			  <a href="https://cszn.github.io/">Kai Zhang</a>,
			  <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>CVPR</em>, 2025, <font color="red"><strong>oral</strong></font> <!-- <a href="https://github.com/yanzq95/DORNet">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  For the first time, we introduce a Degradation Oriented and Regularized Network (DORNet) designed for real-world depth super-resolution, addressing the challenges posed by unconventional and unknown degradations. 
				  The core concept involves estimating implicit degradation representations to achieve effective RGB-D fusion. This degradation learning process is self-supervised.
			  </div>
			  </p>
                </td>
              </tr>

		

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/SigNet/SigNet.png" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2412.19225">Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion</a>
                  </papertitle>
                  <br>
			<strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang</a>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>CVPR</em>, 2025<!-- , <a href="projectpage/TOFDC/index.html">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  We propose a novel degradation-aware framework SigNet that transforms depth completion into depth enhancement for the first time. 
			  SigNet eliminates the mismatch and ambiguity caused by direct convolution over irregularly sampled sparse data. 
			  Meanwhile, it builds a self-supervised degradation bridge between coarse depth and targeted dense depth for effective RGB-D fusion.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


	  <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DHD/show.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2409.07972">Deep Height Decoupling for Precise Vision-based 3D Occupancy Prediction</a>
                  </papertitle>
                  <br>
                        Yuan Wu*</a>, 
			<strong>Zhiqiang Yan* &#9993</strong>, 
			<a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://fpthink.github.io/">Le Hui</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>ICRA</em>, 2025<!--, <a href="projectpage/DHD/index.html">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  For the first time, we introduce the explicit height prior into the vision-based 3D occupancy predition task. Owing to the novel deep height decoupling and sampling stratagy, 
			  our model achieves state-of-the-art performance even with minimal input cost.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DCL/DCL.gif" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2412.11395">Depth-Centric Dehazing and Depth-Estimation from Real-World Hazy Driving Video</a>
                  </papertitle>
                  <br>
                        <a href="https://fanjunkai1.github.io/">Junkai Fan</a>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<strong>Zhiqiang Yan</strong>, 
			<a href="https://cschenxiang.github.io/">Xiang Chen</a>, 
			Shangbin Gao, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>AAAI</em>, 2025<!--, <a href="https://fanjunkai1.github.io/projectpage/DCL/index.html">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			<div class="justify-text">
			  For the first time, we jointly deal with video dehazing and depth estimation. 
			  We propose a novel depth-centric learning framework that combines an atmospheric scattering model (ASM) 
		          with brightness consistency constraint (BCC). 
			  The core idea is to use a shared depth estimation network for both ASM and BCC.
			</div>
		  </p>
                </td>
              </tr>

            </tbody>
          </table>

	    
	    
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DCDepth/dcdepth.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2410.14980">DCDepth: Progressive Monocular Depth Estimation in Discrete Cosine Domain</a>
                  </papertitle>
                  <br>
                        <a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<strong>Zhiqiang Yan</strong>, 
			<a https://scholar.google.com/citations?user=KyHsf00AAAAJ&hl=zh-CN&oi=ao">Junkai Fang</a>, 
			Wanlu Zhu, 
                        <a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>,
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>NeurIPS</em>, 2024<!--, <a href="https://github.com/w2kun/DCDepth">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			<div class="justify-text">
			  DCDepth transforms depth patches into the discrete cosine domain to estimate frequency coefficients, modeling local depth correlations. 
			  The frequency transformation separates depth information into low-frequency (core structure) and high-frequency (details) components. 
			  The progressive strategy predicts low-frequency components first for global context, then refines details with the high-frequency. 
			</div>
		  </p>
                </td>
              </tr>

            </tbody>
          </table>

		
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/MambaLLIE/MambaLLIE.png" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2405.16105">MambaLLIE: Implicit Retinex-Aware Low Light Enhancement with Global-then-Local State Space</a>
                  </papertitle>
                  <br>
                        Jiangwei Weng, 
			<strong>Zhiqiang Yan</strong>, 
			<a href="https://tyshiwo.github.io/">Ying Tai</a>, 
			<a href="https://scholar.google.com/citations?user=oLLDUM0AAAAJ&hl=zh-CN&oi=sra">Jianjun Qian</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a>,
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>
                  <br> <br>
                  <em>NeurIPS</em>, 2024<!--, <a href="https://github.com/MambaLLIE/MambaLLIE">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			<div class="justify-text">
			  MambaLLIE introduces an implicit Retinex-aware low light enhancer with a global-then-local state space design. 
			  This design enables not only the preservation of local dependencies but also comprehensive global modeling. 
			  Extensive experiments show that MambaLLIE significantly surpasses SOTA CNN and Transformer-based methods.
			</div>
		  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/TPVD/TPVD.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://arxiv.org/pdf/2403.15008.pdf">Tri-Perspective View Decomposition for Geometry-Aware Depth Completion</a>
                  </papertitle>
                  <br>
			<strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=I_WdxZwAAAAJ&hl=zh-CN&oi=sra">Yuankai Lin</a>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="https://scholar.google.com/citations?hl=zh-CN&user=anGhGdYAAAAJ">Yupeng Zheng</a>, 
			<a href="https://scholar.google.com/citations?user=kDaztnkAAAAJ&hl=en">Yufei Wang</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>CVPR</em>, 2024, <font color="red"><strong>oral</strong></font><!--, <a href="projectpage/TOFDC/index.html">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  TPVD decomposes 3D point cloud into three views to capture the fine-grained 3D geometry of scenes. TPV Fusion and GSPN are proposed to refine the depth. 
			  Furthermore, we build a novel depth completion dataset named TOFDC, acquired by the time-of-flight (TOF) sensor and the color camera on smartphones.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/LDCNet/LDCNet.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

		  <papertitle><a href="https://link.springer.com/content/pdf/10.1007/s44267-024-00048-9.pdf">Learnable Differencing Center for Nighttime Depth Perception</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?hl=zh-CN&user=anGhGdYAAAAJ">Yupeng Zheng</a>, 
			<a href="https://dengpingfan.github.io/">Deng-Ping Fan</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>Visual Intelligence</em>, 2024<!--, <a href="https://github.com/yanzq95/LDCNet">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  Safe driving in night is critical. Existing image guided methods perform well on daytime depth perception self-driving benchmarks, but struggle in nighttime scenarios with poor visibility and 
			  complex illumination. For the first time, LDCNet exploits to provide a potential solution and introduces two nighttime depth benchmarks.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>
		

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/SPFNet/SPFNet.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2402.13876.pdf">Scene Prior Filtering for Depth Map Super-Resolution</a>
                  </papertitle>
                  <br>
                  <a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang*</a>, 
			  <strong>Zhiqiang Yan*</strong> &#9993, 
			  <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
			  <a href="https://jspan.github.io/">Jinshan Pan</a>, 
			  <a href="https://tyshiwo.github.io/">Ying Tai</a>, 
			  <a href="https://guangweigao.github.io/">Guangwei Gao</a>,
			  <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>arXiv</em>, 2024<!--, <a href="projectpage/SPFNet/index.html">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  To address the issues of texture interference and edge inaccuracy in GDSR, for the first time, SPFNet introduces the priors surface normal and semantic map from large-scale models.
				 As a result, SPFNet achieves state-of-the-art performance.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
		    <img src="images/RigNet++/RigNet++.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2309.00655.pdf">RigNet++: Semantic Assisted Repetitive Image Guided Network for Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://fpthink.github.io/">Le Hui</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>IJCV</em>, 2025
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>
			  <div class="justify-text">
				  On the basis of RigNet, in semantic guidance branch, RigNet++ introduces large-scale model SAM, to supply depth with semantic prior. 
			  In image guidance branch, RigNet++ design a dense repetitive hourglass network (DRHN) to provide powerful contextual instruction for depth 
			  prediction. In addition, RigNet++ proposes a region-aware spatial propagation network (RASPN) for further depth refinement based on the 
			  semantic prior constraint.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>

		
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/SGNet/SGNet_performance.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2312.05799v3.pdf">SGNet: Structure Guided Network via Gradient-Frequency Awareness for Depth Map Super-Resolution</a>
                  </papertitle>
                  <br>
                  <a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang</a>, 
			  <strong>Zhiqiang Yan</strong> &#9993, 
		  	  <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>AAAI</em>, 2024<!--, <a href="projectpage/SGNet/index.html">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
				  SGNet introduces a novel perspective that exploits the gradient and frequency domains for the structure enhancement of DSR task,
				  surpassing the five state-of-the-art methods by 16% (RGB-D-D), 24% (Middlebury), 21% (Lu) and 15% (NYU-v2) in average.
			  </div>
				  
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/AltNeRF/AltNeRF.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2308.10001.pdf">AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization</a>
                  </papertitle>
                  <br>
		  <a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
                  	<strong>Zhiqiang Yan</strong>, 
			<a>Huang Tian</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>AAAI</em>, 2024
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">This paper proposes AltNeRF, a novel framework designed to create resilient NeRF representations using self-supervised monocular depth estimation (SMDE) from monocular videos, 
			  without relying on known camera poses. Extensive experiments showcase the compelling capabilities of AltNeRF in generating high-fidelity and robust novel views that closely resemble reality.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DUL/DUL.png" width="300">&nbsp 
			<img src="images/DUL/DUL-vis.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://openreview.net/pdf?id=0tLjOxqjLS">Distortion and Uncertainty Aware Loss for Panoramic Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="https://shuochenya.github.io/">Shuo Chen &#9993</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a>
                  <br> <br>
                  <em>ICML</em>, 2023
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>
			<div class="justify-text">
				Standard MSE or MAE loss function is commonly used in limited field-of-vision depth completion, treating each pixel equally under a basic assumption that all pixels have same contribution during optimization. 
			  However, the assumption is inapplicable to panoramic data due to its latitude-wise distortion and high uncertainty nearby textures and edges. 
			  To handle these challenges, this paper proposes the distortion and uncertainty aware loss (DUL) that consists of a distortion-aware loss and an uncertainty-aware loss.
			</div>
			</p>
                </td>
              </tr>

            </tbody>
          </table>
		

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
		    <img src="images/DesNet/DesNet-vis.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25415/25187">DesNet: Decomposed Scale-Consistent Network for Unsupervised Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>AAAI</em>, 2023, <font color="red"><strong>oral</strong></font>
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>
			  <div class="justify-text">
				  DesNet first introduces a decomposed scale-consistent learning strategy, which disintegrates the absolute depth into relative depth prediction and global scale estimation, contributing to individual learning benefits. 
		  Extensive experiments show the superiority of DesNet on KITTI benchmark, ranking 1st and surpassing the second best more than 12% in RMSE.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/RigNet/RigNet.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2107.13802.pdf">RigNet: Repetitive Image Guided Network for Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>ECCV</em>, 2022
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>
			  <div class="justify-text">
				  RigNet explores a repetitive design for depth completion to tackle the blurry guidance in image and unclear structure in depth. 
				  Extensive experiments show that RigNet achieves superior or competitive results on KITTI benchmark and NYUv2 dataset.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/MMMPT/MMMPT.jpg" width="300">&nbsp 
			<img src="images/MMMPT/MMMPT-vis.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2203.09855.pdf">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan*</strong>, 
			<a href="http://implus.github.io/">Xiang Li*</a>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>ECCV</em>, 2022
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>
			  <div class="justify-text">
				  For the first time, we enable the masked pre-training in a Convolution-based multi-modal task, instead of the Transformer-based single-modal task. 
		  What's more, we introduce the panoramic depth completion, a new task that facilitates 3D reconstruction.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/IDSR/IDSR.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://drive.google.com/file/d/1qMOQz2sfci_ifRO2aSW5KyyDFKwWvLSW/view">Learning Complementary Correlations for Depth Super-Resolution with Incomplete Data in Real World</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://scholar.google.com/citations?user=qovg0wcAAAAJ&hl=zh-CN&oi=ao">Guangyu Li &#9993</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a>
                  <br> <br>
                  <em>TNNLS</em>, 2022
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>
			  <div class="justify-text">
				  Motivated by pratical applications, this paper introduces a new task, i.e., incomplete depth super-resolution (IDSR), 
				  which recovers dense and high-resolution depth from incomplete and low-resolution one.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/RNW/1.png" width="300">
			<img src="images/RNW/2.png" width="300">
			<img src="images/RNW/3.png" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Regularizing_Nighttime_Weirdness_Efficient_Self-Supervised_Monocular_Depth_Estimation_in_the_ICCV_2021_paper.pdf">Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark</a>
                  </papertitle>
                  <br>
		  <a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang*</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang*</a>, 
                  	<strong>Zhiqiang Yan</strong>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a>Baobei Xu</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>ICCV</em>, 2021<!--, <a href="https://github.com/w2kun/RNW">project page</a> -->
                  <br>
                  <p></p>
                  <p>
			  <div class="justify-text">
			  RNW introduces a nighttime self-supervised monocular depth estimation framework. 
			  The low visibility brings weak textures while the varying illumination breaks brightness-consistency assumption. 
		          To address these problems, RNW proposes the novel Priors-Based Regularization, Mapping-Consistent Image Enhancement, and 
		          Statistics-Based Mask.
			  </div>
			  </p>
                </td>
              </tr>

            </tbody>
          </table>
		

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Honors and Awards</heading>
                  <ul>
		    <li>2023.10, National Scholarship (Top 2%), NJUST;</li>
                  </ul>
                  <ul>
                    <li>2022.10, Hua Wei Scholarship (Top 1%), NJUST;</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Academic Service</heading>
                  <ul>
		    <li>Conference reviewer: CVPR, ICCV, ECCV, NIPS, ICML, ICLR, ICRA, 3DV</li>
                  </ul>
		  <ul>
		    <li>Journal reviewer: TIP, TCSVT, TIV, RAL</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
		

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    This webpage is forked from <a href="https://github.com/fanjunkai1/fanjunkai1.github.io">Junkai Fan</a>. Thanks to him!
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
